<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
    <meta name=viewport content="width=800">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
        /* Color scheme stolen from Sergey Karayev */
        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus,
        a:hover {
            color: #f09228;
            text-decoration: none;
        }

        body,
        td,
        th,
        tr,
        p,
        a {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px
        }

        strong {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
        }

        heading {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 22px;
        }

        papertitle {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
            font-weight: 700
        }

        name {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 32px;
        }

        .one {
            width: 160px;
            height: 160px;
            position: relative;
        }

        .two {
            width: 160px;
            height: 160px;
            position: absolute;
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        .fade {
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        .indented {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 22px;
            font-weight: normal;
            padding-left: 20px;
            margin-bottom: -40px;
            margin-top: -0px;
        }

        .first-author-paper {
            display: table-row;
        }

        .collaborative-paper {
            display: none;
        }

        span.highlight {
            background-color: #ffffd0;
        }

        button {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            background-color: #ffffff;
            /* 修改背景为白色 */
            border: 1px solid #ddd;
            /* 添加浅灰色边框 */
            color: #888;
            /* 修改文本为灰色 */
            padding: 10px 15px;
            font-size: 14px;
            border-radius: 5px;
            cursor: pointer;
            transition: background-color 0.2s, transform 0.2s;
            display: inline-block;
        }

        button:hover {
            background-color: #f2f2f2;
            /* 鼠标悬停时的背景颜色 */
            transform: scale(1.05);
        }

        button:focus {
            outline: none;
        }

        button.active {
            background-color: #d3d3d3;
            /* Gray color for active button */
        }

        #first-author-btn,
        {
        width: 180px;
        height: 40px;
        }

        #collaborative-btn {
            width: 120px;
            height: 40px;
        }

        .up-one-line {
            margin-top: 0em;
        }

        .papers-heading {
            margin-bottom: 10px;
            /* Adjust as needed */
        }

        .papers-table {
            margin-top: -90px;
            /* Adjust as needed */
        }
    </style>

    <link rel="icon" type="image/jpg" href="images/seal_icon.jpg">
    <title>Yan Ding</title>
    <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
    <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet'
        type='text/css'>
</head>

<!-- Add JavaScript to toggle visibility -->
<script type="text/javascript">
    function togglePapers(category) {
        var firstAuthorPapers = document.getElementsByClassName('first-author-paper');
        var collaborativePapers = document.getElementsByClassName('collaborative-paper');
        var preprintPapers = document.getElementsByClassName('preprint-paper');

        // Hide all rows first
        for (var i = 0; i < firstAuthorPapers.length; i++) {
            firstAuthorPapers[i].style.display = 'none';
        }
        for (var i = 0; i < collaborativePapers.length; i++) {
            collaborativePapers[i].style.display = 'none';
        }
        for (var i = 0; i < preprintPapers.length; i++) {
            preprintPapers[i].style.display = 'none';
        }

        // Show rows based on the selected category
        if (category === 'first-author') {
            for (var i = 0; i < firstAuthorPapers.length; i++) {
                firstAuthorPapers[i].style.display = 'table-row';
            }
        } else if (category === 'collaborative') {
            for (var i = 0; i < collaborativePapers.length; i++) {
                collaborativePapers[i].style.display = 'table-row';
            }
        } else if (category === 'preprint') {
            for (var i = 0; i < preprintPapers.length; i++) {
                preprintPapers[i].style.display = 'table-row';
            }
        }
    }
</script>

<body>
    <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
        <tr>
            <td>
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                    <tr>
                        <td width="100%" valign="middle" class="papers-heading">
                            <p align="center">
                                <name>Yan Ding</name>
                            </p>
                            <p>I am a Researcher at Shanghai AI Laboratory.
                                I completed my PhD in Computer Science at the State University of New York (SUNY),
                                Binghamton, in May 2024.
                                I have been supported by grants from the Ford Motor Company for 4.5 years during my PhD
                                studies.
                                I also got the Academic Excellence in Computer Science (PhD) at Binghamton University.
                                <!-- I was supervised by Professor <a href="http://www.cs.cqu.edu.cn/info/1322/6092.htm">Chao
                                    Chen</a>, during my master's program. -->
                                I received my M.S. in Computer Science in 2019, and got my B.S. in Mechanical
                                Engineering in 2016 from Chongqing University, China.
                                The master's thesis is awarded the Outstanding Thesis of Chongqing City.
                                <!-- %, is available via this <a href="./project/PDF/dissertation_master.pdf">Link</a> -->
                                <!-- %directed by Professor <a href="https://scholar.google.com/citations?user=ahUibskAAAAJ&hl=en&oi=ao">Xuelong Li</a> -->
                            </p>

                            <p><span style="color: red;">I am seeking interns for Embodied AI and Robotics!</span> Feel
                                free to contact me at yding25@binghamton.edu.
                            </p>
                        </td>
                        <td width="33%">
                            <img src="img/headshot3.jpg" width="256" height="264">
                        </td>
                    </tr>
                </table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                    <tr>
                        <td width="100%" valign="middle" class="papers-heading">
                            <heading>Research Direction</heading>
                            <p>The current research interests include:</p>
                            <ul>
                                <li><span style="color: #1772d0;">Spatial Intelligence for Robotics</span>: Empowering
                                    Robots to Understand the Real World
                                </li>
                                <li><span style="color: #1772d0;">Skill Learning for Robotics</span>: Enabling Robots to
                                    Transform the Real World
                                </li>
                            </ul>
                            <p>with a particular emphasis on their applications in the context of <span
                                    style="color: #1772d0;">mobile manipulators (MoMa)</span>.</p>
                        </td>
                    </tr>
                </table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                    <tr>
                        <td width="100%" valign="middle" class="papers-heading">
                            <heading>Robot Family</heading>
                            <p>
                                <strong>BestMan X</strong> reflects my vision for these robots to be the best assistants
                                for humans.
                            </p>
                            </p>
                            <img src="/img/BestMan_robots.jpg" alt="real_robots" style="width: 500px;">
                            </p>

                            <br>

                            <heading>BestMan World</heading>
                            <p>
                                It is a comprehensive “BestMan” world, encompassing open data, code, simulators,
                                hardware, and my aspirations. For more information, please visit
                                <a href="https://bestmanrobot.com/" target="_blank">[Link]</a>.
                            </p>


                            <br>

                            <heading>Robotic Tool</heading>
                            <p>My team has multiple robots. Therefore, we are developing an open-source robotic tool
                                called <a href="http://bestmanrobot.com/">BestMan</a>. This tool supports development
                                both in simulation and on real machines. By using a unified framework, BestMan
                                facilitates rapid development, helping researchers save significant time. (Note: BestMan
                                is still under construction.)
                            </p>
                            <p>This project encompasses various sub-projects (selected):
                            <ul>
                                <li>
                                    <a href="https://github.com/AutonoBot-Lab/BestMan_Pybullet">BestMan_Pybullet</a>
                                    <img src="https://img.shields.io/github/stars/AutonoBot-Lab/BestMan_Pybullet?style=social"
                                        alt="GitHub stars">
                                </li>
                                <li>
                                    <a href="https://github.com/yding25/BestMan_Flexiv">BestMan_Arm (Private)</a>
                                    <!-- <img src="https://img.shields.io/github/stars/yding25/BestMan_Flexiv?style=social" alt="GitHub stars"> -->
                                </li>
                                <li>
                                    <a href="https://github.com/yding25/BestMan_Xarm">BestMan_Wheel (Private)</a>
                                    <!-- <img src="https://img.shields.io/github/stars/yding25/BestMan_Xarm?style=social" alt="GitHub stars"> -->
                                </li>
                                <li>
                                    <a href="https://github.com/yding25/BestMan_Xarm">BestMan_Hand (Private)</a>
                                    <!-- <img src="https://img.shields.io/github/stars/yding25/BestMan_Xarm?style=social" alt="GitHub stars"> -->
                                </li>
                                <li>
                                    <a href="https://github.com/yding25/BestMan_Xarm">BestMan_Humanoid (Private)</a>
                                    <!-- <img src="https://img.shields.io/github/stars/yding25/BestMan_Xarm?style=social" alt="GitHub stars"> -->
                                </li>
                            </ul>

                            <br>

                            <heading>Robotic Demos</heading>
                            <p>
                                Xiaohongshu Channel: <a
                                    href="https://www.xiaohongshu.com/user/profile/5e3d92d00000000001000a14?xhsshare=CopyLink&appuid=5e3d92d00000000001000a14&apptime=1727694511&share_id=9e781b4ef2ef424cbfbb2fd64d59be0d">444988405</a>
                            </p>
                            <!-- <p>
                                YouTube Channel: <a href="https://www.youtube.com/@yanding1760">&#64;yanding1760</a>
                            </p> -->
                            <p>
                                WeChat Video Channel (Scan it using WeChat):
                            </p>
                            <p>
                                <img src="/img/WeChat_QR.jpg" alt="WeChat QR Code" style="width: 200px;">
                            </p>
                        </td>
                    </tr>
                </table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                    <td width="100%" valign="middle" class="papers-heading">
                        <heading>Publication</heading>
                    </td>
                    <td width="100%" valign="middle" class="papers-heading">
                        <button id="first-author-btn"
                            onclick="togglePapers('first-author')">First(*)/Corresponding(#)</button>
                    </td>
                    <td width="100%" valign="middle" class="papers-heading">
                        <button id="collaborative-btn" onclick="togglePapers('collaborative')">Collaborative</button>
                    </td>
                    <td width="100%" valign="middle" class="papers-heading">
                        <button id="preprint-btn" onclick="togglePapers('preprint')">Preprint</button>
                    </td>
                </table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10" class="papers-table"
                    style="margin-top: -11em;">
        </tr>

<tr class="first-author-paper preprint-paper"></tr>
        <td width="25%">
            <div class="one">
                <div class="two"><img src='img/MoMa-Kitchen.png' width="160" vspace="5"></div>
            </div>
        </td>
        <br>
        <td valign="middle" width="75%">
            <a>
                <papertitle>MoMa-Kitchen: A 100K+ Benchmark for Affordance-Grounded Last-Mile Navigation in Mobile Manipulation</papertitle>
            </a>
            <br>
           Pingrui Zhang<sup>1,2*</sup>, Xianqiang Gao<sup>2,3∗</sup>, Yuhan Wu<sup>3</sup>,Kehui Liu<sup>2,4</sup>, Dong Wang<sup>2</sup>, Zhigang Wang<sup>2</sup>, Bin Zhao<sup>2,4</sup>, <strong>Yan Ding<sup>2†</sup></strong>, Xuelong Li<sup>5</sup>
            <br>
            <em>Under Review</em>
            <br>
            [<a href="">Paper</a>]
            [<a href="https://momakitchen.github.io/">Project</a>]
            <br>
            <p></p>
            <p> We present MoMa-Kitchen, a benchmark dataset with over 100k auto-generated samples featuring affordance-grounded manipulation positions and egocentric RGB-D data, and propose NavAff, a lightweight model that learns optimal navigation termination for seamless manipulation transitions. Our approach generalizes across diverse robotic platforms and arm configurations, addressing the critical gap between navigation proximity and manipulation readiness in mobile manipulation.
            </p>
        </td>
                    
        <tr class="first-author-paper preprint-paper"></tr>
        <td width="25%">
            <div class="one">
                <div class="two"><img src='img/agibot-world.png' width="160" vspace="5"></div>
            </div>
        </td>
        <br>
        <td valign="middle" width="75%">
            <a>
                <papertitle>AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent
                    Embodied Systems</papertitle>
            </a>
            <br>
            Team AgiBot-World
            <br>
            <em>Under Review</em>
            <br>
            [<a href="https://opendrivelab.com/assets/file/AgiBot_World_Colosseo.pdf">Paper</a>]
            [<a href="https://opendrivelab.com/blog/agibot-world/">Project</a>]
            <br>
            <p></p>
            <p> Introducing AgiBot World Colosseo, an open-sourced large-scale manipulation platform comprising data,
                models, benchmarks and ecosystem.
                AgiBot World stands out for its unparalleled scale and diversity compared to prior counterparts.
                A suite of 100 dual-arm humanoid robots is deployed.
                We further propose a generalist policy (GO-1) with the latent action planner.
                It is trained across diverse data corpus with a scalable performance of 32% gain compared to prior arts.
            </p>
        </td>

        <tr class="first-author-paper preprint-paper"></tr>
        <td width="25%">
            <div class="one">
                <div class="two"><img src='img/BestMan.png' width="160" vspace="5"></div>
            </div>
        </td>
        <br>
        <td valign="middle" width="75%">
            <a>
                <papertitle>BestMan: A Modular Mobile Manipulator Platform for Embodied AI with Unified
                    Simulation-Hardware APIs</papertitle>
            </a>
            <br>
            Kui Yang*, Nieqing Cao*, <strong>Yan Ding#</strong>, Chao Chen#
            <br>
            <em>FCS Letter</em>
            <br>
            [<a href="https://arxiv.org/pdf/2410.13407">Paper</a>]
            [<a href="https://bestmansim.com/">Project</a>]
            <br>
            <p></p>
            <p> To address these challenges, we develop the BestMan platform based on the PyBullet
                simulator, with the following key contributions:
                1) Integrated Multilevel Skill Chain to Address Multilevel Technical Complexity;
                2) Highly Modular Design for Expandability and Algorithm Integration;
                3)Unified Interfaces for Simulation and Real Devices to Address Interface Heterogeneity;
                4)Decoupling Software from Hardware to Address Hardware Diversity. </p>
        </td>

        <tr class="first-author-paper preprint-paper"></tr>
        <td width="25%">
            <div class="one">
                <div class="two"><img src='img/fastumi.jpg' width="160" vspace="5"></div>
            </div>
        </td>
        <br>

        <td valign="middle" width="75%">
            <a>
                <papertitle>FastUMI: A Scalable and Hardware-Independent Universal Manipulation Interface and Dataset
                </papertitle>
            </a>
            <br>
            Zhaxizhuoma*, Kehui Liu*, Chuyue Guan*, Zhongjie Jia*, Ziniu Wu*, Xin Liu*, Tianyu Wang**, Shuai Liang**,
            Pengan Chen**, Pingrui Zhang**, Haoming Song, Delin Qu,
            Dong Wang, Zhigang Wang, Nieqing Cao, <strong>Yan Ding*#</strong>, Bin Zhao#, Xuelong Li
            <br>
            <em>Under Review</em>
            <br>
            [<a href="https://arxiv.org/pdf/2409.19499">Paper</a>]
            [<a href="https://fastumi.com/">Project</a>]
            <br>
            <p></p>
            <p> In this work, we introduce Fast-UMI, an interface-mediated manipulation system comprising two key
                components: a handheld device operated by humans for data collection and a robot-mounted device used
                during policy inference.
                This system offers an efficient and user-friendly tool for robotic learning data acquisition. </p>
        </td>
        </tr>

        <tr class="first-author-paper preprint-paper">
            <td width="25%">
                <div class="one">
                    <div class="two"><img src='img/alignbot.png' width="160" vspace="5"></div>
                </div>
            </td>
            <br>

            <td valign="middle" width="75%">
                <a>
                    <papertitle>AlignBot: Aligning VLM-powered Customized Task Planning with User Reminders Through
                        Fine-Tuning for Household Robots</papertitle>
                </a>
                <br>
                Zhaxizhuoma*, Pengan Chen*, Ziniu Wu*, Jiawei Sun, Dong Wang, Peng Zhou, Nieqing Cao, <strong>Yan
                    Ding#</strong>, Bin Zhao, Xuelong Li
                <br>
                <em>ICRA 2025</em>
                <br>
                [<a href="https://arxiv.org/pdf/2409.11905">Paper</a>]
                [<a href="https://yding25.com/AlignBot/">Project</a>]
                <!-- [<a
                                    href="https://yding25.com/AlignBot/">Video</a>]
                                [<a
                                    href="">Code</a>] -->
                <br>
                <p></p>
                <p> This paper presents AlignBot, a novel framework designed to optimize VLM-powered customized task
                    planning for household robots by effectively aligning with user reminders.
                    AlignBot employs a fine-tuned LLaVA-7B model, functioning as an adapter for GPT-4o. This adapter
                    model internalizes diverse forms of user reminders-such as personalized preferences, corrective
                    guidance, and contextual assistance into structured instruction-formatted cues that prompt GPT-4o in
                    generating customized task plans. </p>
            </td>
        </tr>

        <tr class="collaborative-paper">
            <td width="25%">
                <div class="one">
                    <div class="two"><img src='img/dkprompt.png' width="160" vspace="20"></div>
                </div>
            </td>
            <br>

            <td valign="middle" width="75%">
                <a>
                    <papertitle>DKPROMPT: Domain Knowledge Prompting Vision-Language Models for Open-World Planning
                    </papertitle>
                </a>
                <br>
                Xiaohan Zhang, Zainab Altaweel, Yohei Hayamizu, <strong>Yan Ding</strong>, Saeid Amiri, Hao Yang, Andy
                Kaminski, Chad Esselink, Shiqi Zhang
                <br>
                <em>Under Review</em>
                <br>
                [<a href="https://arxiv.org/pdf/2406.17659">Paper</a>]
                <br>
            </td>
        </tr>

        <tr class="collaborative-paper">
            <td width="25%">
                <div class="one">
                    <div class="two"><img src='img/mtech.png' width="160" vspace="20"></div>
                </div>
            </td>
            <br>

            <td valign="middle" width="75%">
                <a>
                    <papertitle>A Survey of Optimization-based Task and Motion Planning: From Classical To Learning
                        Approaches</papertitle>
                </a>
                <br>
                Zhigen Zhao, Shuo Chen, <strong>Yan Ding</strong>, Ziyi Zhou, Shiqi Zhang, Danfei Xu, Ye Zhao
                <br>
                <em>IEEE/ASME Transactions on Mechatronics, 2024</em>
                <br>
                [<a href="https://arxiv.org/pdf/2404.02817">Paper</a>]
                <br>
            </td>
        </tr>

        <tr class="first-author-paper preprint-paper">
            <td width="25%">
                <div class="one">
                    <div class="two"><img src='img/moma_pos.jpg' width="160" vspace="5"></div>
                </div>
            </td>
            <br>

            <td valign="middle" width="75%">
                <a>
                    <papertitle>MoMa-Pos: Where Should Mobile Manipulators Stand in Cluttered Environment Before Task
                        Execution?</papertitle>
                </a>
                <br>
                Beichen Shao*, <strong>Yan Ding*#</strong>, Xingchen Wang, Xuefeng Xie, Fuqiang Gu, Jun Luo, Chao Chen#
                <br>
                <em>Under Review</em>
                <br>
                [<a href="https://arxiv.org/pdf/2403.19940.pdf">Paper</a>]
                [<a href="https://yding25.com/MoMa-Pos/">Project</a>]
                <!-- [<a
                                    href="https://yding25.com/MoMa-Pos/static/assets/videos/demos/demo1.mp4">Video</a>]
                                [<a
                                    href="https://github.com/AutonoBot-Lab/Project_MoMa-Pos">Code</a>] -->
                <br>
                <p></p>
                <p> Mobile manipulators always need to determine feasible base positions prior to carrying out
                    navigation-manipulation tasks.
                    Real-world environments are often cluttered with various furniture, obstacles, and dozens of other
                    objects. Efficiently computing base positions poses a challenge.
                    In this work, we introduce a framework named MoMa-Pos to address this issue.</p>
            </td>
        </tr>

        <tr class="first-author-paper">
            <td width="25%">
                <div class="one">
                    <div class="two"><img src='img/llm-grop.jpg' width="160" vspace="5"></div>
                </div>
            </td>
            <br>

            <td valign="middle" width="75%">
                <a>
                    <papertitle>Task and Motion Planning with
                        Large Language Models for Object
                        Rearrangement</papertitle>
                </a>
                <br>
                <strong>Yan Ding</strong>, Xiaohan Zhang, Chris Paxton, Shiqi Zhang
                <br>
                <em>International Conference on Intelligent Robots and Systems (IROS), 2023</em>
                <br>
                [<a href="https://arxiv.org/pdf/2303.06247.pdf">Paper</a>]
                [<a href="https://sites.google.com/view/llm-grop">Project</a>]
                <!-- [<a
                                    href="https://youtu.be/8q5WPkABfNE">Video</a>]
                                [<a
                                    href="https://colab.research.google.com/drive/1cSqoSc6Gk9KM9p-GwHSIIL5VfZICGW3B?usp=sharing">Code</a>] -->
                <br>
                <p></p>
                <p>LLM-GROP is a method that uses prompting to extract commonsense knowledge about object configurations
                    from a large language model and instantiates them with a task and motion planner, allowing for
                    successful and efficient multi-object rearrangement in various environments using a mobile
                    manipulator.</p>
            </td>
        </tr>

        <tr class="collaborative-paper">
            <td width="25%">
                <div class="one">
                    <div class="two"><img src='img/Chelsea.png' width="160" vspace="20"></div>
                </div>
            </td>
            <br>

            <td valign="middle" width="75%">
                <a>
                    <papertitle>ARDIE: AR, Dialogue, and Eye Gaze Policies for Human-Robot Collaboration</papertitle>
                </a>
                <br>
                Chelsea Zou, Kishan Chandan, <strong>Yan Ding</strong>, Shiqi Zhang
                <br>
                <em>ICRA Workshop on CoPerception: Collaborative
                    Perception and Learning, 2023</em>
                <br>
                [<a href="https://arxiv.org/pdf/2305.04685.pdf">Paper</a>]
            </td>
        </tr>

        <tr class="collaborative-paper">
            <td width="25%">
                <div class="one">
                    <div class="two"><img src='img/s30.png' width="160" vspace="20"></div>
                </div>
            </td>
            <br>

            <td valign="middle" width="75%">
                <a>
                    <papertitle>Symbolic State Space Optimization for Long Horizon Mobile Manipulation Planning
                    </papertitle>
                </a>
                <br>
                Xiaohan Zhang, Yifeng Zhu, <strong>Yan
                    Ding</strong>, Yuqian Jiang, Yuke Zhu, Peter
                Stone, and Shiqi Zhang
                <br>
                <em>International Conference on Intelligent
                    Robots and Systems (IROS), 2023</em>
                <br>
                [<a href="https://arxiv.org/pdf/2307.11889.pdf">Paper</a>]
            </td>
        </tr>

        <tr class="collaborative-paper">
            <td width="25%">
                <div class="one">
                    <div class="two"><img src='img/UAI.jpg' width="160" vspace="20"></div>
                </div>
            </td>
            <br>

            <td valign="middle" width="75%">
                <a>
                    <papertitle>Learning to Reason about Contextual Knowledge for Planning under Uncertainty
                    </papertitle>
                </a>
                <br>
                Cheng Cui, Saeid Amiri, <strong>Yan
                    Ding</strong>, Xingyue Zhan, Shiqi Zhang
                <br>
                <em>The Conference on Uncertainty in Artificial
                    Intelligence (UAI), 2023</em>
                <br>
                [<a href="https://proceedings.mlr.press/v216/cui23a/cui23a.pdf">Paper</a>]
            </td>
        </tr>

        <tr class="preprint-paper">
            <td width="25%">
                <div class="one">
                    <div class="two"><img src='img/ORLA.png' width="160" vspace="20"></div>
                </div>
            </td>
            <td valign="middle" width="75%">
                <a>
                    <papertitle>ORLA*: Mobile Manipulator-Based Object Rearrangement with LazyA</papertitle>
                </a>
                <br>
                Kai Gao*, Zhaxizhuoma*, <strong>Yan Ding</strong>, Shiqi Zhang, Jingjin Yu
                <br>
                <em>ICRA 2025</em>
                <br>
                [<a href="https://arxiv.org/pdf/2309.13707.pdf">Paper</a>]
                <br>
                <p></p>
                <p>In this research, we propose ORLA*, which leverages delayed (lazy) evaluation in searching for a
                    high-quality object pick and place sequence that considers both end-effector and mobile robot base
                    travel.</p>
            </td>
        </tr>

        <tr class="collaborative-paper">
            <td width="25%">
                <div class="one">
                    <div class="two"><img src='img/tpvqa.png' width="160" vspace="20"></div>
                </div>
            </td>
            <br>

            <td valign="middle" width="75%">
                <a>
                    <papertitle>Grounding Classical Task Planners via Vision-Language Models</papertitle>
                </a>
                <br>
                Xiaohan Zhang, <strong>Yan Ding</strong>, Saeid
                Amiri, Hao Yang, Andy Kaminski, Chad Esselink,
                and Shiqi Zhang
                <br>
                <em>ICRA Workshop on Robot Execution Failures
                    and Failure Management Strategies, 2023</em>
                <br>
                [<a href="https://arxiv.org/pdf/2304.08587.pdf">Paper</a>]
                <br>
            </td>
        </tr>

        <tr class="first-author-paper">
            <td width="25%">
                <div class="one">
                    <div class="two"><img src='img/cowp_real.png' width="160" vspace="50"></div>
                </div>
            </td>
            <td valign="middle" width="75%">
                <a>
                    <papertitle>Integrating Action Knowledge and
                        LLMs for Task Planning and Situation
                        Handling in Open Worlds</papertitle>
                </a>
                <br>
                <strong>Yan Ding</strong>, Xiaohan Zhang, Saeid
                Amiri, Nieqing Cao, Hao Yang, Chad Esselink,
                Shiqi Zhang
                <br>
                <em>Autonomous Robots (accepted)</em>
                <br>
                [<a href="https://arxiv.org/pdf/2305.17590.pdf">Paper</a>]
                [<a href="https://cowplanning.github.io/">Project</a>]
                [<a href="https://youtu.be/HtxlXSzY5VQ">Video</a>]
                [<a href="https://github.com/yding25/GPT-Planner">Code</a>]
                <br>
                <p></p>
                <p>The paper introduces a new algorithm (COWP)
                    that uses task-oriented common sense
                    extracted from Large Language Models to help
                    robots handle unforeseen situations and
                    complete complex tasks in an open world,
                    with better success rates than previous
                    algorithms.</p>
            </td>
        </tr>

        <tr class="first-author-paper">
            <td width="25%">
                <div class="one">
                    <div class="two"><img src='img/tmoc.png' width="160" vspace="30"></div>
                </div>
            </td>
            <td valign="middle" width="75%">
                <a>
                    <papertitle>Learning to Ground Objects for
                        Robot Task and Motion
                        Planning</papertitle>
                </a>
                <br>
                <strong>Yan Ding</strong>, Xiaohan Zhang,
                Xingyue Zhan, Shiqi Zhang
                <br>
                <em>IEEE Robotics and Automation Letters
                    (RA-L)</em>, 2022
                <br>
                [<a href="https://arxiv.org/pdf/2202.06674.pdf">Paper</a>]
                [<a href="https://yding25.github.io/project/TMOC/TMOC.html">Project</a>]
                [<a href="https://github.com/yding25/TMOC">Code</a>]
                [<a href="https://www.youtube.com/embed/3ijtbbeCQho">Presentation</a>]
                <br>
                <p></p>
                <p>The paper presents a new robot planning
                    algorithm, TMOC, which can handle complex
                    real-world scenarios without prior knowledge
                    of object properties by learning them
                    through a physics engine, outperforming
                    existing algorithms.</p>
            </td>
        </tr>

        <tr class="collaborative-paper">
            <td width="25%">
                <div class="one">
                    <div class="two"><img src='img/grop_real.png' width="160" vspace="20"></div>
                </div>
            </td>
            <td valign="middle" width="75%">
                <a>
                    <papertitle>Visually Grounded Task and
                        Motion Planning for Mobile
                        Manipulation</papertitle>
                </a>
                <br>
                Xiaohan Zhang, Yifeng Zhu, <strong>Yan
                    Ding</strong>, Yuke Zhu, Peter Stone, and
                Shiqi Zhang
                <br>
                <em>International Conference on Robotics and
                    Automation (ICRA)</em>, 2022
                <br>
                [<a href="https://arxiv.org/pdf/2202.10667.pdf">Paper</a>]
                [<a href="https://sites.google.com/view/grop-tamp">Project</a>]
            </td>
        </tr>

        <tr class="collaborative-paper">
            <td width="25%">
                <div class="one">
                    <div class="two"><img src='img/howie.png' width="160" vspace="20"></div>
                </div>
            </td>
            <td valign="middle" width="75%">
                <a>
                    <papertitle>Task and Situation Structures
                        for Case-Based Planning</papertitle>
                </a>
                <br>
                Hao Yang, Tavan Eftekhar, Chad Esselink,
                <strong>Yan Ding</strong>, Shiqi Zhang
                <br>
                <em>International Conference on Case-Based
                    Reasoning (ICCBR)</em>, 2021
                <br>
                [<a href="https://link.springer.com/chapter/10.1007/978-3-030-86957-1_18">Paper</a>]
            </td>
        </tr>

        <tr class="first-author-paper">
            <td width="25%">
                <div class="one">
                    <div class="two"><img src='img/tmpud.png' width="160" vspace="20"></div>
                </div>
            </td>
            <td valign="middle" width="75%">
                <a>
                    <papertitle>Task-Motion Planning for Safe
                        and Efficient Urban Driving</papertitle>
                </a>
                <br>
                <strong>Yan Ding</strong>, Xiaohan Zhang,
                Xingyue Zhan, Shiqi Zhang
                <br>
                <em>International Conference on Intelligent
                    Robots and Systems (IROS)</em>, 2020.
                <br>
                [<a href="https://arxiv.org/pdf/2003.03807">Paper</a>]
                [<a href="https://yding25.com/TMPUD/website/TMPUD.html">Project</a>]
                [<a href="https://github.com/yding25/TMPUD">Code</a>]
                [<a href="https://www.youtube.com/watch?v=8NHQYUqMyoI">Demo</a>]
                [<a href="https://youtu.be/k-Pcnx8zgxE">Presentation</a>]
                <br>
                <p></p>
                <p>Autonomous vehicles need to balance
                    efficiency and safety when planning tasks
                    and motions, and the algorithm Task-Motion
                    Planning for Urban Driving (TMPUD) enables
                    communication between planners for optimal
                    performance.</p>
            </td>
        </tr>

        <tr class="first-author-paper">
            <td width="25%">
                <div class="one">
                    <div class="two"><img src='img/DVAT.png' width="160" vspace="20"></div>
                </div>
            </td>
            <td valign="middle" width="75%">
                <a>
                    <papertitle>DAVT: an error-bounded vehicle
                        trajectory data representation and
                        compression framework</papertitle>
                </a>
                <br>
                Chao Chen*, <strong>Yan Ding*</strong>, Suiming
                Guo, Yasha Wang
                <br>
                <em>IEEE TVT</em>, 2020.
                <br>
                [<a
                    href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9163296&casa_token=ZbgAMop0EiYAAAAA:BVnLHMxb3jaLs0Ukmq_RTszRARtuiPh5qg51GXNgxmmOsr5GbGna31OvxG6WnB8gfnyEJQUb">PDF</a>]
                <br>
                <p></p>
                <p>DAVT proposes a mobile edge computing
                    solution for vehicle trajectory data
                    compression, which reduces data at the
                    source and lowers communication and storage
                    costs, using three compressors for distance,
                    acceleration, velocity, and time data parts,
                    and outperforms other baselines according to
                    evaluation results.</p>
            </td>
        </tr>

        <tr class="first-author-paper">
            <td width="25%">
                <div class="one">
                    <div class="two"><img src='img/VTracer.png' width="160" vspace="20"></div>
                </div>
            </td>
            <td valign="middle" width="75%">
                <a>
                    <papertitle>VTracer: When online vehicle
                        trajectory compression meets mobile edge
                        computing</papertitle>
                </a>
                <br>
                Chao Chen*, <strong>Yan Ding*</strong>, Zhu
                Wang, Junfeng Zhao, Bin Guo, Daqing Zhang
                <br>
                <em>IEEE Systems Journal</em>, 2019.
                <br>
                [<a href="https://hal.science/hal-02321015/document">PDF</a>]
                <br>
                <p></p>
                <p>This paper proposes an online trajectory
                    compression framework that uses SD-Matching
                    for GPS alignment and HCC for compression,
                    and demonstrates its effectiveness and
                    efficiency using real-world datasets in
                    Beijing and deployment in Chongqing.</p>
            </td>
        </tr>

        <tr class="first-author-paper">
            <td width="25%">
                <div class="one">
                    <div class="two"><img src='img/trajcompressor.png' width="160" vspace="20"></div>
                </div>
            </td>
            <td valign="middle" width="75%">
                <a>
                    <papertitle>TrajCompressor: An Online
                        Map-matching-based Trajectory
                        Compression Framework Leveraging Vehicle
                        Heading Direction and
                        Change</papertitle>
                </a>
                <br>
                Chao Chen*, <strong>Yan Ding*</strong>, Xuefeng
                Xie, Shu Zhang, Zhu Wang, Liang Feng
                <br>
                <em>IEEE TITS</em>, 2019.
                <br>
                [<a href="https://ieeexplore.ieee.org/abstract/document/8697124">PDF</a>]
                <br>
                <p></p>
                <p>This paper presents an online trajectory
                    compression framework for reducing storage,
                    communication, and computation issues caused
                    by massive and redundant vehicle trajectory
                    data, consisting of two phases: online
                    trajectory mapping and trajectory
                    compression, using Spatial-Directional
                    Matching and Heading Change Compression
                    algorithms respectively, which have been
                    evaluated with real-world datasets in
                    Beijing and deployed in Chongqing, showing
                    higher accuracy and efficiency compared to
                    state-of-the-art algorithms.
                </p>
            </td>
        </tr>

        <tr class="first-author-paper">
            <td width="25%">
                <div class="one">
                    <div class="two"><img src='img/api.png' width="160" vspace="20"></div>
                </div>
            </td>
            <td valign="middle" width="75%">
                <a>
                    <papertitle>Fuel Consumption Estimation of
                        Potential Driving Paths by Leveraging
                        Online Route APIs</papertitle>
                </a>
                <br>
                Chao Chen*, <strong>Yan Ding*</strong>, Xuefeng
                Xie, Xuefeng Xie, Zhikai Yang
                <br>
                <em>Green, Pervasive, and Cloud Computing: 13th
                    International Conference (GPC)</em>, 2018.
                <br>
                [<a href="https://link.springer.com/chapter/10.1007/978-3-030-15093-8_7">PDF</a>]
                <br>
                <p></p>
                <p>This paper proposes a fuel consumption model
                    based on GPS trajectory and OBD-II data,
                    which can estimate the fuel usage of driving
                    paths and help drivers choose fuel-efficient
                    routes to reduce greenhouse gas and
                    pollutant emissions.</p>
            </td>
        </tr>

        <tr class="first-author-paper">
            <td width="25%">
                <div class="one">
                    <div class="two"><img src='img/threestage.png' width="160" vspace="20"></div>
                </div>
            </td>
            <td valign="middle" width="75%">
                <a>
                    <papertitle>A three-stage online
                        map-matching algorithm by fully using
                        vehicle heading direction</papertitle>
                </a>
                <br>
                Chao Chen*, <strong>Yan Ding*</strong>, Xuefeng
                Xie, Shu Zhang
                <br>
                <em>Journal of Ambient Intelligence and
                    Humanized Computing</em>, 2018.
                <br>
                [<a href="https://link.springer.com/article/10.1007/s12652-018-0760-0">PDF</a>]
                <br>
                <p></p>
                <p>The SD-Matching algorithm proposes a
                    three-stage approach to improve the accuracy
                    and speed of online map-matching by
                    incorporating vehicle heading direction
                    data.</p>
            </td>
        </tr>

        <tr class="first-author-paper">
            <td width="25%">
                <div class="one">
                    <div class="two"><img src='img/greenplanner.png' width="160" vspace="20"></div>
                </div>
            </td>
            <td valign="middle" width="75%">
                <a>
                    <papertitle>Greenplanner: Planning
                        personalized fuel-efficient driving
                        routes using multi-sourced urban
                        data</papertitle>
                </a>
                <br>
                <strong>Yan Ding*</strong>, Chao Chen*, Shu
                Zhang, Bin Guo, Zhiwen Yu, Yasha Wang
                <br>
                <em>IEEE PerCom</em>, 2017.
                <br>
                [<a
                    href="https://www.researchgate.net/profile/Chao-Chen-82/publication/311588334_GreenPlanner_Planning_Personalized_Fuel-efficient_Driving_Routes_using_Multi-sourced_Urban_Data/links/5a67db75a6fdcce9c106ed92/GreenPlanner-Planning-Personalized-Fuel-efficient-Driving-Routes-using-Multi-sourced-Urban-Data.pdf">PDF</a>]
                <br>
                <p></p>
                <p>Greenhouse gas emissions from vehicles in
                    modern cities is a significant problem, but
                    recommending fuel-efficient routes to
                    drivers through a personalized fuel
                    consumption model can help alleviate this
                    issue, as demonstrated by the successful
                    implementation of GreenPlanner in Beijing,
                    which achieved a mean fuel consumption error
                    of less than 7% and an average savings of
                    20% fuel consumption for suggested
                    routes.</p>
            </td>
        </tr>

        <script src="https://cdn.jsdelivr.net/gh/yasserelsaid/chatbot@latest/index.min.js"
            id="yding25-com-8yejwvr6d"></script>
    </table>
    </td>
    </tr>
    </table>

    <table width="100%" cellspacing="0" cellpadding="20" border="0" align="center">
        <tbody>
            <tr>
                <td>
                    <br>
                    <p align="center">
                        <font size="2">
                            Template from <a href="https://github.com/jonbarron/jonbarron_website">here.</a>
                        </font>
                    </p>
                </td>
            </tr>
        </tbody>
    </table>

    <div id="clustrmaps-container" style="width: 400px; height: 400px; margin: 0 auto; text-align: center;">
        <script type="text/javascript" id="clustrmaps"
            src="https:////clustrmaps.com/map_v2.js?d=WYdWih066J3wTpH_iNl0ozrF76XRIfsDmIJ7adgimc4&cl=ffffff&w=a"></script>
    </div>
    <!-- <script type="text/javascript" id="clstr_globe" src="https://clustrmaps.com/globe.js?d=WYdWih066J3wTpH_iNl0ozrF76XRIfsDmIJ7adgimc4"></script> -->
</body>
